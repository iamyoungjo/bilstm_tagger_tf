{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from itertools import count\n",
    "import random\n",
    "\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from tflearn.datasets import imdb\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.embedding_ops import embedding\n",
    "from tflearn.layers.recurrent import bidirectional_rnn, BasicLSTMCell\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# format of files: each line is \"word1/tag2 word2/tag2 ...\"\n",
    "train_file=\"tagged_train.txt\"\n",
    "test_file=\"tagged_test.txt\"\n",
    "\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, w2i=None):\n",
    "        if w2i is None: w2i = defaultdict(int)\n",
    "        self.w2i = dict(w2i)\n",
    "        self.i2w = {i:w for w,i in w2i.items()}\n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus):\n",
    "        w2i = defaultdict(int)\n",
    "        for sent in corpus:\n",
    "            [w2i[word] for word in sent]\n",
    "        return Vocab(w2i)\n",
    "\n",
    "    def size(self): return len(self.w2i.keys())\n",
    "\n",
    "def read(fname):\n",
    "    \"\"\"\n",
    "    Read a POS-tagged file where each line is of the form \"word1/tag2 word2/tag2 ...\"\n",
    "    Yields lists of the form [(word1,tag1), (word2,tag2), ...]\n",
    "    \"\"\"\n",
    "    with open(fname) as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip().split()\n",
    "            sent = [tuple(x.rsplit(\"/\",1)) for x in line]\n",
    "            yield sent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train=list(read(train_file))\n",
    "words=[]\n",
    "tags=[]\n",
    "chars=set()\n",
    "tagset = set()\n",
    "wc=Counter()\n",
    "for sent in train:\n",
    "    for w,p in sent:\n",
    "        words.append(w)#.lower())#float(w.split(\"word\")[1]))\n",
    "        if(len(w)>25):\n",
    "            print(\"over 25!\", w)\n",
    "        tags.append(p)\n",
    "        tagset.add(p)\n",
    "        chars.update(w)#.lower())\n",
    "        wc[w]+=1\n",
    "# words.append(\"_UNK_\")\n",
    "# chars.add(\"<*>\")\n",
    "\n",
    "\n",
    "# words_t=[]\n",
    "# tags_t=[]\n",
    "#wc_t=Counter()\n",
    "# for sent in test:\n",
    "#     for w,p in sent:\n",
    "#         words_t.append(w)\n",
    "#         tags_t.append(p)#(float(p.split(\"tag\")[1]))\n",
    "#         tagset.add(p)\n",
    "#         chars.update(w)\n",
    "        #wc_t[w]+=1\n",
    "# # words_t.append(\"_UNK_\")\n",
    "# # chars_t.add(\"<*>\")\n",
    "\n",
    "# vw = Vocab.from_corpus([words]) \n",
    "# vt = Vocab.from_corpus([tags])\n",
    "# vc = Vocab.from_corpus([chars])\n",
    "# # UNK = vw.w2i[\"_UNK_\"]\n",
    "\n",
    "# nwords = vw.size()\n",
    "# ntags  = vt.size()\n",
    "# nchars  = vc.size()\n",
    "\n",
    "def w2v(word, d):\n",
    "\twvec = []\n",
    "\tfor c in word:\n",
    "\t\twvec.append(d[c])\n",
    "\treturn wvec\n",
    "\n",
    "dictionary = {i:j for j,i in enumerate(list(chars))}\n",
    "tagdict = {i:j for j,i in enumerate(list(tagset))}\n",
    "\n",
    "trainW = []\n",
    "for word in words:\n",
    "\t#wvec = np.zeros(14)\n",
    "\t#wvec[int(word)-1] = 1.0\n",
    "\ttrainW.append(w2v(word, dictionary))\n",
    "trainW = np.array(trainW)# .reshape(len(words), 50000)\n",
    "\n",
    "trainT = []\t\n",
    "for tag in tags:\n",
    "# \ttvec = np.zeros(10)\n",
    "# \ttvec[int(tag)-1] = 1.0\n",
    "\ttrainT.append(tagdict[tag])\n",
    "trainT = np.array(trainT)#.reshape(len(tags), 1)\n",
    "\n",
    "# testW = []\n",
    "# for word in words_t:\n",
    "# # \twvec = np.zeros(14)\n",
    "# # \twvec[int(word)-1] = 1.0\n",
    "# \ttestW.append(w2v(word, dictionary))\n",
    "# testW = np.array(testW)# .reshape(len(words_t), 50000)\n",
    "\n",
    "# testT = []\t\n",
    "# for tag in tags_t:\n",
    "# # \ttvec = np.zeros(10)\n",
    "# # \ttvec[int(tag)-1] = 1.0\n",
    "# \ttestT.append(tagdict[tag])\n",
    "# testT = np.array(testT)#.reshape(len(tags_t), 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '$',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting labels to binary vectors\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "charvocab = len(chars)\n",
    "maxbound = 25\n",
    "valid = 5000\n",
    "#Pre-processing\n",
    "# Sequence padding\n",
    "trainW = pad_sequences(trainW, maxlen=maxbound, value=0.)\n",
    "testW = trainW[-valid:]\n",
    "trainW = trainW[:-valid]\n",
    "# Converting labels to binary vectors\n",
    "trainT = to_categorical(trainT, nb_classes=len(tagset))\n",
    "testT = trainT[-valid:]\n",
    "trainT = trainT[:-valid]\n",
    "#testT = to_categorical(testT, nb_classes=len(tagset))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#dd[chars.pop() for i in range(len(chars))\n",
    "# np.array(testT[3])\n",
    "#trainT.shape\n",
    "len(tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7040  | total loss: \u001b[1m\u001b[32m0.45511\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 010 | loss: 0.45511 - acc: 0.8540 | val_loss: 0.56175 - val_acc: 0.8388 -- iter: 45000/45000\n",
      "Training Step: 7040  | total loss: \u001b[1m\u001b[32m0.45511\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 010 | loss: 0.45511 - acc: 0.8540 | val_loss: 0.56175 - val_acc: 0.8388 -- iter: 45000/45000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# DyNet Starts\n",
    "# TFlearn starts\n",
    "\n",
    "\n",
    "\n",
    "# Data preprocessing\n",
    "# print(\"nwords: \", nwords)\n",
    "# print(\"ntags: \", ntags)\n",
    "# print(\"train[:5] = \", train[:5])\n",
    "\n",
    "# Network building (char-level)\n",
    "net = input_data(shape=[None, maxbound])\n",
    "net = embedding(net, input_dim=charvocab, output_dim=256)\n",
    "net = bidirectional_rnn(net, BasicLSTMCell(256), BasicLSTMCell(256))\n",
    "net = dropout(net, 0.5)\n",
    "net = fully_connected(net, len(tagset), activation='softmax')\n",
    "net = regression(net, optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Training\n",
    "\n",
    "\n",
    "\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "\n",
    "model.fit(trainW, trainT, validation_set=(testW, testT), show_metric=True, batch_size=64)\n",
    "\n",
    "# ##example prediction after model set\n",
    "# print(\"testW[0]: \", testW[0])\n",
    "# print(\"Prediction of testW[0]: \", model.predict(testW[0]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  7, 13, 11, 63],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        46, 27, 24, 24, 16, 32, 64, 20],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0, 53],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0, 51, 23, 13],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0, 13, 67],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0, 51, 14, 63],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 54, 63,\n",
       "        67, 63, 32, 54, 18, 32, 51, 20],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0, 53],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0, 18, 32, 54],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0, 18, 24, 20, 13],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 16, 32,\n",
       "        51, 34, 13, 54, 27, 30, 63, 54],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        46, 27, 24, 24, 16, 32, 64, 20],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0, 51, 13],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0, 39, 63, 20, 20, 45],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0, 12, 18, 34, 13, 45],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0, 53],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0, 18],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0, 55, 18, 32],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 55,\n",
       "        63, 32, 51, 16, 13, 32, 63, 54],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0, 16, 32]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testW[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "Hope Pullings , two of the defendants , and also introduced Pullings to Jessy Maroy , a man mentioned in the indictment but not indicted . Buaford Robinson , 23 , of 7026 Stewart Av. , a CTA bus driver\n",
      "\n",
      "Prediction: \n",
      "['Hope/NN-TL', 'Pullings/NP', ',/,', 'two/CD', 'of/IN', 'the/AT', 'defendants/NNS', ',/,', 'and/CC', 'also/RB', 'introduced/VBN', 'Pullings/NP', 'to/IN', 'Jessy/NP', 'Maroy/NP', ',/,', 'a/AT', 'man/NN', 'mentioned/VBN', 'in/IN', 'the/AT', 'indictment/NN', 'but/CC', 'not/*', 'indicted/VBD', './.', 'Buaford/NP', 'Robinson/NP', ',/,', '23/CD', ',/,', 'of/IN', '7026/CD', 'Stewart/NP', 'Av./NN-TL', ',/,', 'a/AT', 'CTA/NN', 'bus/CC', 'driver/NN']\n",
      "\n",
      "Answer: \n",
      "['Hope/NP', 'Pullings/NP', ',/,', 'two/CD', 'of/IN', 'the/AT', 'defendants/NNS', ',/,', 'and/CC', 'also/RB', 'introduced/VBD', 'Pullings/NP', 'to/IN', 'Jessy/NP', 'Maroy/NP', ',/,', 'a/AT', 'man/NN', 'mentioned/VBN', 'in/IN', 'the/AT', 'indictment/NN', 'but/CC', 'not/*', 'indicted/VBN', './.', 'Buaford/NP', 'Robinson/NP', ',/,', '23/CD', ',/,', 'of/IN', '7026/CD', 'Stewart/NP', 'Av./NN-TL', ',/,', 'a/AT', 'CTA/NN', 'bus/NN', 'driver/NN']\n"
     ]
    }
   ],
   "source": [
    "modelfile = \"..\"\n",
    "#for i in range(10):\n",
    "    #print( np.argmax(model.predict(testW[:10])[i]))\n",
    "\n",
    "def i2t(index):\n",
    "    return list(tagset)[index]\n",
    "def is2w(indices):\n",
    "    switch = 0\n",
    "    word = []\n",
    "    for i in range(maxbound):\n",
    "        if(indices[i]!=0):\n",
    "            switch = 1\n",
    "        if(switch == 1):\n",
    "            word.append(list(chars)[indices[i]])\n",
    "    \n",
    "    return \"\".join(word)    \n",
    "    \n",
    "#print([i2t(np.argmax(a)) for a in testT[:20]])\n",
    "#print([i2t(np.argmax(a)) for a in model.predict(testW[:20])])\n",
    "window = 40\n",
    "print(\"Input:\")\n",
    "print(\" \".join([is2w(testW[:window][i]) for i in range(len(testW[:window]))]))\n",
    "print(\"\\nPrediction: \")\n",
    "print([is2w(testW[:window][i])+\"/\" +i2t(np.argmax(a)) for i, a in enumerate(model.predict(testW[:window]))])\n",
    "print(\"\\nAnswer: \")\n",
    "print([is2w(testW[:window][i])+\"/\" +i2t(np.argmax(a)) for i, a in enumerate(testT[:window])])\n",
    "model.save(modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
